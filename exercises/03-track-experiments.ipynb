{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment tracking with MLflow ðŸ“ˆ\n",
    "\n",
    "Running experiments in data science projects is a common task. We have run one in the previous notebook. However, it is often difficult to keep track of all the experiments that have been run by you or by your colleagues, and to compare the results between them.\n",
    "\n",
    "In this notebook, we show how to use the MLflow library for experiment tracking.\n",
    "\n",
    "Now, after training our model. We will use an MLflow server to:\n",
    "- Log our model configuration (features, architecture, etc.)\n",
    "- Log our test score\n",
    "- Log the model\n",
    "- Log additional artifacts, such as figures\n",
    "- *Register* a model if the model is better than the rest\n",
    "\n",
    "> **Note** An MLflow server has been spun up for you in the background. It is hosted somewhere in the cloud, so everyone can access it. **In case the server breaks**, you may run the following in your terminal to spin up an mlflow server locally:\n",
    "```bash\n",
    "mlflow server --default-artifact-root ./mlruns --host 0.0.0.0\n",
    "```\n",
    "> Make sure, in that case, to use the tracking uri (e.g. `http://0.0.0.0:5000`) instead of the one provided below and throughout the rest of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import plotly_express as px\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "import sklearn.linear_model\n",
    "import sklearn.ensemble\n",
    "import sklearn.tree\n",
    "import mlflow\n",
    "from typing import Dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by connecting to the MLflow server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_uri = \"http://20.4.198.104:5000\"\n",
    "\n",
    "# Fill in your name below. This will make sure that \n",
    "# whatever you log to mlflow will be associated to you\n",
    "your_name = \"< fill in your name here >\"\n",
    "\n",
    "# mlflow config\n",
    "os.environ[\"LOGNAME\"] = your_name\n",
    "mlflow.set_tracking_uri(tracking_uri)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And prepare a few lines of code to allow us to experiment with different models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/turbine-data.csv\"\n",
    "data = pd.read_csv(data_path).set_index(\"timestamp\")\n",
    "data.index = pd.to_datetime(data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"wind_speed\",\n",
    "    \"wind_direction\",\n",
    "    \"is_curtailed\",\n",
    "]\n",
    "\n",
    "# Drop data with missing values\n",
    "data_without_na = data.dropna() \n",
    "\n",
    "X = data_without_na[features]  # Our model's input\n",
    "y = data_without_na[\"active_power\"]  # Target values\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    shuffle=False,\n",
    "    # use (only) 10% of all data for training\n",
    "    train_size=0.1  # don't change this number\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.pipeline.make_pipeline(\n",
    "    # Preprocessing\n",
    "    # sklearn.preprocessing.StandardScaler(),\n",
    "    # sklearn.preprocessing.PolynomialFeatures(degree=3),\n",
    "\n",
    "    # Model\n",
    "    sklearn.linear_model.LinearRegression(),\n",
    "    # sklearn.linear_model.Ridge(),\n",
    "    # sklearn.tree.DecisionTreeRegressor(),\n",
    "    # sklearn.ensemble.RandomForestRegressor(),\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "score = model.score(X_test, y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure to log to MLflow\n",
    "\n",
    "# Apply model to all data\n",
    "data.loc[X.index, \"predictions\"] = model.predict(X)\n",
    "\n",
    "fig = px.line(\n",
    "    data[X_test.index.min():],\n",
    "    y=[\"active_power\", \"predictions\"],\n",
    "    title=\"Predicted and true generated power\"\n",
    ")\n",
    "fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's log the results to the MLflow server!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"code-breakfast-september-2023\")\n",
    "\n",
    "model_name = \"turbine-model\"   # feel free to come up with your own name\n",
    "\n",
    "# Exercise: what does `mlflow.start_run()` do?\n",
    "\n",
    "with mlflow.start_run():\n",
    "    \n",
    "    # Grab the run_id, we'll use it later\n",
    "    run_id = mlflow.active_run().info.run_id\n",
    "        \n",
    "    mlflow.log_param(\"features\", \", \".join(features))\n",
    "    mlflow.log_param(\"pipeline_steps\", str(model))\n",
    "    mlflow.log_param(\"model_name\", model_name)\n",
    "    \n",
    "    # Exercise: log the model using mlflow.sklearn.log_model\n",
    "    #           and use the model_name defined above\n",
    "    # ... \n",
    "    \n",
    "    # Exercise: log the model test score as `test_score` using mlflow.log_metric\n",
    "    # ...\n",
    "    \n",
    "    # Exercise: log a figure with mlflow.log_figure (tip: use a `.html` extension as file name)\n",
    "    # ...\n",
    "    \n",
    "# Exercise: try to improve your model above by changing the pipeline and features and log the results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visit the MLflow server:\n",
    "\n",
    "[http://20.4.198.104:5000](http://20.4.198.104:5000)\n",
    "\n",
    "Find your run!\n",
    "\n",
    "ðŸŽ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering a model \n",
    "\n",
    "Now that we have run a few experiments, we can register the best model we have found so far.\n",
    "\n",
    "You can do so either in the user interface (UI) of MLflow, or programmatically. Check out the server explore how you can register a model in the UI.\n",
    "\n",
    "Below we show how to do it in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions to get the all runs and scores from MLflow\n",
    "\n",
    "def get_runs(experiment_name: str = \"Default\"):\n",
    "    \"\"\"Gets all runs from an experiment in MLflow\"\"\"\n",
    "    \n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    experiment = client.get_experiment_by_name(experiment_name)\n",
    "    runs = client.search_runs(experiment.experiment_id)\n",
    "    return runs\n",
    "\n",
    "\n",
    "def get_scores_per_run_id(experiment_name: str = \"Default\") -> Dict[str, float]:\n",
    "    \"\"\"Returns dictionary as {\"<run_id>\" : <test_score>} with all runs from an experiment in MLflow\"\"\"\n",
    "    \n",
    "    runs = get_runs(experiment_name)\n",
    "    scores = {run.info.run_id: run.data.metrics[\"test_score\"] for run in runs if \"test_score\" in run.data.metrics}\n",
    "    return scores\n",
    "\n",
    "\n",
    "def check_best_score(run_id: str, experiment_name: str = \"Default\") -> bool:\n",
    "    \"\"\"Checks if the given run_id has the best score in the experiment\"\"\"\n",
    "    \n",
    "    scores = get_scores_per_run_id(experiment_name)\n",
    "\n",
    "    # if there are no scores, raise an error\n",
    "    if len(scores) == 0:\n",
    "        raise ValueError(f\"No `test_score` scores found for experiment '{experiment_name}'\")\n",
    "    \n",
    "    # score is always 'the best score' if there is only one run\n",
    "    if len(scores) == 1:\n",
    "        return True\n",
    "\n",
    "    run_score = scores[run_id]\n",
    "    other_scores = [score for (i, score) in scores.items() if i != run_id]\n",
    "    return run_score > min(other_scores)\n",
    "\n",
    "scores = get_scores_per_run_id(experiment_name=\"code-breakfast-september-2023\")\n",
    "\n",
    "if len(scores):\n",
    "    print(f\"Number runs found with a `test_score`: {len(scores)}\")\n",
    "    print(f\"Average score: {sum(scores.values()) / len(scores)}\")\n",
    "    print(f\"Best score: {max(scores.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_model(run_id: str):\n",
    "    \"\"\"Register the model with the given run_id and model_name to MLflow\"\"\"\n",
    "    \n",
    "    # Get model_name from run_id\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    run = client.get_run(run_id)\n",
    "    model_name = run.data.params[\"model_name\"]\n",
    "    \n",
    "    model_uri = f\"runs:/{run_id}/{model_name}\"\n",
    "    mlflow.register_model(model_uri, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: \n",
    "# Do a conditional check if the model has the best score\n",
    "# and register it to MLflow it it does. We are using the \n",
    "# `run_id` we have defined before.\n",
    "\n",
    "if check_best_score(run_id, experiment_name=\"code-breakfast-september-2023\"):\n",
    "    print(\"Your model has the best score!\")\n",
    "    register_model(run_id)\n",
    "else:\n",
    "    print(f\"Your score of {score} is not better than the max score of {max(scores.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Registered models can be moved from staging to production, simply in the MLflow UI. <br>\n",
    "Check out the \"Models\" tab in MLflow and explore a bit.\n",
    "\n",
    "In the next notebook, we'll show how you can load a model from production for inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
